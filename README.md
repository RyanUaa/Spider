# Python3 Spider

---

## 什么是网络爬虫

网络爬虫始于一张被称作种子的统一资源地址（URL）列表。当网络爬虫访问这些统一资源定位器时，它们会甄别出页面上所有的超链接，并将它们写入一张“待访列表”，即所谓爬行疆域。此疆域上的URL将会被按照一套策略循环来访问。如果爬虫在执行的过程中复制归档和保存网站上的信息，这些档案通常储存，使他们可以较容易的被查看。阅读和浏览他们存储的网站上并即时更新的信息，这些被存储的网页又被称为“快照”。越大容量的网页意味着网络爬虫只能在给予的时间内下载越少部分的网页，所以要优先考虑其下载。高变化率意味着网页可能已经被更新或者被取代。一些服务器端软件生成的URL（统一资源定位符）也使得网络爬虫很难避免检索到重复内容。

但是互联网的资源卷帙浩繁，这也意味着网络爬虫只能在一定时间内下载有限数量的网页，因此它需要衡量优先级的下载方式。有时候网页出现、更新和消失的速度很快，也就是说网络爬虫下载的网页在几秒后就已经被修改或甚至删除了。这些都是网络爬虫设计师们所面临的两个问题。

再者，服务器端软件所生成的统一资源地址数量庞大，以至网络爬虫难免也会采集到重复的内容。根据超文本传输协议，无尽组合的参数所返回的页面中，只有很少一部分确实传回正确的内容。例如：数张快照陈列室的网站，可能通过几个参数，让用户选择相关快照：其一是通过四种方法对快照排序，其二是关于快照分辨率的的三种选择，其三是两种文件格式，另加一个用户可否提供内容的选择，这样对于同样的结果会有48种（4*3*2）不同的统一资源地址与其关联。这种数学组合替网络爬虫造成了麻烦，因为它们必须越过这些无关脚本变化的组合，寻找不重复的内容。

*[维基百科](https://zh.wikipedia.org/wiki/%E7%B6%B2%E8%B7%AF%E7%88%AC%E8%9F%B2)*

*[百度百科](https://baike.baidu.com/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB)*

## 写了什么

本文以库的形式安排，先从基本库摸清学透，缓慢加深、循序渐进，这是本人Python3爬虫的学习过程、平时的笔记、代码、总结心得。文件目录基本上都是一个章节中一篇记录(.md文件),一份代码(.py文件，在完成库的讲解之后一个完整的爬虫小项目)。尽量详细的记录了我用Python写爬虫的探索。

我也只是菜鸟一个，有可能罗里吧嗦，有可能不知所云，有任何问题，请与我联系。

> - Email：jian2binary@gmail.com.
> - QQ：3264722570

## 写在前面

如果把该笔记看作教程，该教程适用于零基础，当然有一定基础更好，有些涉及的知识我只是提到，但并不是无关紧要，弄明白了再进行下一步才是最有效的学习，实践得真知，一起进步。

## 配置

如果你想与作者得到的结果一样

> Ubantu 18.04.1 
 
> Python 3.7

第三方库用到的时候会提及版本。